# Vision Service - Hybrid Image Analysis

## Overview

We implemented a **hybrid** computer vision system:
- **TensorFlow Lite** (local): Fast object detection, works offline
- **Azure Computer Vision** (cloud): Rich contextual descriptions, internet-dependent

The system **always** uses TFLite locally and enriches with Azure when connected.

## Architecture

### Clean Architecture in 3 Layers

```
src/vision/
â”œâ”€â”€ domain/              # Pure business logic
â”‚   â”œâ”€â”€ entities/
â”‚   â”‚   â”œâ”€â”€ DetectedObject.ts      # Detected object (label, confidence, position)
â”‚   â”‚   â””â”€â”€ SceneDescription.ts    # Complete scene description
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ SceneDescriptionGenerator.ts  # Generates natural language text
â”‚   â””â”€â”€ value-objects/
â”‚       â””â”€â”€ LabelTranslations.ts   # EN-ES dictionary (80 COCO categories)
â”‚
â”œâ”€â”€ application/         # Use cases
â”‚   â”œâ”€â”€ ports/
â”‚   â”‚   â”œâ”€â”€ ICameraService.ts      # Interface for camera
â”‚   â”‚   â””â”€â”€ IVisionService.ts      # Interface for vision analysis
â”‚   â””â”€â”€ use-cases/
â”‚       â””â”€â”€ AnalyzeSceneUseCase.ts # Orchestrates: capture + analysis + description
â”‚
â”œâ”€â”€ infrastructure/      # Concrete implementations
â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â”œâ”€â”€ expo/
â”‚   â”‚   â”‚   â””â”€â”€ ExpoCameraAdapter.ts        # Implements ICameraService with expo-camera
â”‚   â”‚   â”œâ”€â”€ tflite/
â”‚   â”‚   â”‚   â””â”€â”€ TFLiteVisionAdapter.ts      # Implements IVisionService with TFLite (local)
â”‚   â”‚   â”œâ”€â”€ azure/
â”‚   â”‚   â”‚   â””â”€â”€ AzureVisionAdapter.ts       # Implements IVisionService with Azure CV (cloud)
â”‚   â”‚   â”œâ”€â”€ hybrid/
â”‚   â”‚   â”‚   â””â”€â”€ HybridVisionAdapter.ts      # Combines TFLite + Azure
â”‚   â”‚   â””â”€â”€ voice/
â”‚   â”‚       â””â”€â”€ VisionServiceBridge.ts      # Connects vision module with voice module
â”‚   â””â”€â”€ services/
â”‚
â””â”€â”€ presentation/        # UI and hooks
    â”œâ”€â”€ components/
    â”‚   â””â”€â”€ CameraCapture.tsx       # Invisible component with active camera
    â””â”€â”€ hooks/
        â””â”€â”€ useVisionService.ts     # React hook to access service
```

## Hybrid Strategy: TFLite + Azure

### Why hybrid?

| Aspect | TFLite (Local) | Azure Computer Vision |
|--------|----------------|----------------------|
| **Speed** | âš¡ 200-500ms | ğŸŒ 1-2s (network dependent) |
| **Offline** | âœ… Works without internet | âŒ Requires internet |
| **Privacy** | âœ… 100% private | âš ï¸ Sends image to cloud |
| **Detection** | âœ… 80 COCO objects | âœ… 10,000+ objects |
| **Description** | ğŸ“ Basic template | ğŸ¨ Contextual and natural |
| **Cost** | âœ… Always free | âœ… 5,000/month free |

### Implemented Strategy

```
User: "iris describe"
       â†“
1. ALWAYS runs TFLite (local)
   â†’ Detects basic objects
   â†’ Generates structured description
   â†’ Fast and offline
       â†“
2. Is internet available? â†’ Yes
       â†“
3. Enriches with Azure Computer Vision
   â†’ Deep contextual analysis
   â†’ More natural description
   â†’ Combines TFLite objects + Azure description
       â†“
4. Returns best of both worlds
```

### Advantages of This Strategy

1. **Always works**: TFLite guarantees offline functionality
2. **Improves with internet**: Azure adds context and richness
3. **Fast**: TFLite responds first, Azure enriches after
4. **Economical**: 5,000 analyses free/month with Azure

## Execution Flow

### When user says "iris describe" (WITH INTERNET)

```
1. ProcessCommandUseCase.handleDescribe()
           â†“
2. VisionServiceBridge.analyzeScene()
           â†“
3. AnalyzeSceneUseCase.execute()
           â†“
4. ExpoCameraAdapter.capturePhoto()
   â†’ Takes photo with expo-camera
   â†’ Returns local URI
           â†“
5. HybridVisionAdapter.analyzeImage()
           â†“
   5a. TFLiteVisionAdapter.analyzeImage()
       â†’ Loads COCO-SSD model (if not loaded)
       â†’ Runs inference on image (200-500ms)
       â†’ Returns: { objects: [...], naturalDescription: "basic template" }
           â†“
   5b. Checks internet connection
       â†’ NetInfo.fetch() â†’ isConnected: true
           â†“
   5c. AzureVisionAdapter.analyzeImage()
       â†’ Sends image to Azure Computer Vision API
       â†’ Azure analyzes full context (1-2s)
       â†’ Returns: { naturalDescription: "rich and contextual description" }
           â†“
   5d. Combines results
       â†’ objects: from TFLite (with coordinates)
       â†’ naturalDescription: from Azure (more natural)
       â†’ Best of both worlds
           â†“
6. ExpoSpeechSynthesizer.speak(description)
   â†’ TTS reads enriched description
```

### When user says "iris describe" (NO INTERNET)

```
1-4. [Same as above]
           â†“
5. HybridVisionAdapter.analyzeImage()
           â†“
   5a. TFLiteVisionAdapter.analyzeImage()
       â†’ Detects objects locally
       â†’ Returns: { objects: [...], naturalDescription: "template" }
           â†“
   5b. Checks internet connection
       â†’ NetInfo.fetch() â†’ isConnected: false
       â†’ âš ï¸ Skips Azure, uses only TFLite
           â†“
   5c. Returns local result
       â†’ objects: from TFLite
       â†’ naturalDescription: generated by SceneDescriptionGenerator
           â†“
6. ExpoSpeechSynthesizer.speak(description)
   â†’ TTS reads local description
```

## Key Components

### 1. DetectedObject (Entity)

```typescript
interface DetectedObject {
  label: string;           // "person" (English, from model)
  labelEs: string;         // "persona" (Spanish, translated)
  confidence: number;      // 0.92 (92% confidence)
  boundingBox: {
    x: 0.2,                // Normalized position (0-1)
    y: 0.3,
    width: 0.4,
    height: 0.5
  };
  position: 'center';      // Calculated: center, left, right, top, bottom
  size: 'large';           // Calculated: large, medium, small
}
```

### 2. SceneDescription (Entity)

```typescript
interface SceneDescription {
  objects: DetectedObject[];
  timestamp: Date;
  confidence: number;      // Average of confidences
  naturalDescription: string;  // "I see 2 people and a chair"
  imageUri?: string;       // URI of analyzed photo
}
```

### 3. SceneDescriptionGenerator (Domain Service)

Converts technical data to natural language:

**Input:**
```json
[
  {"label": "person", "confidence": 0.95},
  {"label": "person", "confidence": 0.92},
  {"label": "chair", "confidence": 0.88}
]
```

**Output:**
```
"I see 2 people and a chair in the center"
```

**Features:**
- Groups objects by type ("2 people" instead of "person, person")
- Uses correct plurals ("chairs", "people")
- Filters by minimum confidence (default: 50%)
- Describes position of main objects
- Handles edge cases (0 objects, 1 object, many objects)

### 4. TFLiteVisionAdapter (Infrastructure)

**Current state:** Mock implementation
- Simulates detections for testing without physical device
- Returns test data: person, chair, laptop

**When building on real device:**
1. Download COCO-SSD model (~5MB)
2. Uncomment react-native-fast-tflite code
3. Load model on app startup
4. Run real inference

**See:** `/mobile/assets/models/README.md` for instructions

### 5. ExpoCameraAdapter (Infrastructure)

- Implements ICameraService using expo-camera
- Handles permissions automatically
- Captures high-quality photos (optimized to 640x640 for TFLite)
- Uses invisible CameraView in background

## Description Examples: TFLite vs Azure

### Example 1: Office

**TFLite Detection (local):**
```
Objects: person (0.92), laptop (0.88), chair (0.76), cup (0.65)
Description: "I see a person, a laptop, a chair and a cup"
```

**Azure Computer Vision (enriched):**
```
Objects: [same as TFLite with coordinates]
Description: "A person working in a modern office with a laptop on the desk and a coffee cup beside"
```

### Example 2: Street

**TFLite Detection (local):**
```
Objects: car (0.91), car (0.88), person (0.83), traffic light (0.72)
Description: "I see 2 cars, a person and a traffic light"
```

**Azure Computer Vision (enriched):**
```
Objects: [same as TFLite]
Description: "An urban street with two parked cars and a person crossing at the traffic light"
```

### Example 3: No Internet (TFLite only)

```
[No internet connection]
Objects: dog (0.89), person (0.85), ball (0.67)
Description: "I see a dog, a person and a ball"
```

## Translations (80 COCO Categories)

The COCO-SSD model detects 80 categories. All are translated to English:

| Category EN | Category ES |
|-------------|-------------|
| person      | persona     |
| car         | coche       |
| chair       | silla       |
| laptop      | portÃ¡til    |
| cup         | taza        |
| bottle      | botella     |
| ...         | ...         |

**See:** `LabelTranslations.ts` for complete list

## Integration with Voice Module

### VisionServiceBridge

Adapter that connects both modules while maintaining separation:

```typescript
// Voice module expects:
interface VisionService {
  analyzeScene(): Promise<SceneAnalysis>;
  isReady(): boolean;
}

// Vision module provides:
class AnalyzeSceneUseCase {
  execute(): Promise<SceneDescription>;
}

// Bridge connects both:
class VisionServiceBridge implements VisionService {
  async analyzeScene(): Promise<SceneAnalysis> {
    const description = await this.analyzeSceneUseCase.execute();
    return {
      description: description.naturalDescription,
      objects: description.objects.map(...)
    };
  }
}
```

## Usage from React

### Hook: useVisionService

```typescript
function MyComponent() {
  const { visionService, isReady } = useVisionService({
    preload: true  // Pre-load models on mount
  });

  useEffect(() => {
    if (isReady) {
      console.log('Vision service ready!');
    }
  }, [isReady]);

  return <>{/* ... */}</>;
}
```

### Integration in App.tsx

```typescript
function App() {
  // 1. Initialize vision service
  const { visionService, cameraAdapter } = useVisionService({ preload: true });

  // 2. Pass to voice commands
  return (
    <>
      <HomeScreen visionService={visionService} />
      <CameraCapture onAdapterReady={cameraAdapter.setCameraRef.bind(cameraAdapter)} />
    </>
  );
}
```

## Expected Performance

### First execution:
- Model loading: ~500-1000ms
- Photo capture: ~200-300ms
- TFLite inference: ~200-500ms
- Total: ~1-2 seconds

### Subsequent executions:
- Model in memory (already loaded)
- Capture: ~200-300ms
- Inference: ~200-500ms
- Total: ~500-800ms

### Implemented optimizations:
- âœ… Pre-load models on app startup
- âœ… Camera always active (not started/stopped each time)
- âœ… Image resize to 640x640 (optimal for TFLite)
- âœ… Minimum confidence filter (50%)
- âœ… Cache of last description (for "iris repeat")

## App Size

- **TFLite Models:** ~5-6 MB (COCO-SSD)
- **Vision module code:** ~50 KB
- **Total additional:** ~5-6 MB to APK/IPA

## Next Steps

### To test on real device:

1. **Download COCO-SSD model:**
   ```bash
   cd mobile/assets/models
   curl -L -o coco_ssd_mobilenet_v1.tflite \
     "https://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/metadata/2?lite-format=tflite"
   ```

2. **Configure react-native-fast-tflite:**
   ```bash
   cd mobile
   npx expo prebuild --clean
   npx expo run:android  # or run:ios
   ```

3. **Update TFLiteVisionAdapter:**
   - Uncomment real TFLite code
   - Remove mock data

4. **Test command:**
   - Say "iris describe"
   - Verify photo capture
   - Verify object detection
   - Verify description in English

### Future improvements (optional):

- [ ] Add MobileNet V2 for scene classification
- [ ] Detect dominant colors
- [ ] OCR to read text in images
- [ ] Face detection (BlazeFace)
- [ ] "Continuous exploration" mode (analyzes every 5 seconds)
- [ ] Description history with timestamps

## Testing

### Mock for development without device:

The current TFLiteVisionAdapter includes mock data:

```typescript
// MOCK detections
return [
  { label: 'person', confidence: 0.92, ... },
  { label: 'chair', confidence: 0.85, ... },
  { label: 'laptop', confidence: 0.78, ... },
];
```

This enables:
- âœ… Develop UI without needing device
- âœ… Test description generation
- âœ… Validate complete end-to-end flow
- âœ… Write unit tests

### Existing unit tests:

```bash
npm test -- vision
```

Tests:
- SceneDescriptionGenerator with various scenarios
- Label translations
- Position and size calculation
- Confidence filtering

## Troubleshooting

### "Vision service not ready"
â†’ Models are still loading. Wait a few seconds or call `warmUp()`.

### "Camera permission denied"
â†’ Go to Settings > Iris > Permissions > Camera > Allow

### "No objects detected"
â†’ Scene is too dark or no objects recognized by COCO (80 categories)

### Build fails with TFLite
â†’ Make sure you've run `npx expo prebuild` before `expo run:android`

## Referencias

- [COCO Dataset](https://cocodataset.org/) - 80 categorÃ­as detectables
- [TensorFlow Lite](https://www.tensorflow.org/lite) - ML on-device
- [expo-camera docs](https://docs.expo.dev/versions/latest/sdk/camera/)
- [react-native-fast-tflite](https://github.com/mrousavy/react-native-fast-tflite)
