# Mobile App Guidelines

This AGENTS.md extends the root guidelines with mobile-specific patterns for Iris.

## Skills

Skills available for mobile development:

<!-- PROJECT-SKILLS:START - Do not remove. Auto-generated by skill-sync -->
| Skill | Description | URL |
|-------|-------------|-----|
| `react-native` | React Native patterns and mobile best practices | [SKILL.md](../skills/react-native/SKILL.md) |
| `expo` | Expo SDK, config plugins, and EAS Build | [SKILL.md](../skills/expo/SKILL.md) |
| `tensorflow-lite` | On-device ML with TensorFlow Lite | [SKILL.md](../skills/tensorflow-lite/SKILL.md) |
<!-- PROJECT-SKILLS:END -->

### Auto-invoke Skills

When performing these actions, ALWAYS invoke the corresponding skill FIRST:

| Action | Skill |
|--------|-------|
| -- | `atomic-design` |
| -- | `clean-architecture` |
| -- | `expo` |
| -- | `react` |
| -- | `react-native` |
| -- | `tensorflow-lite` |
| -- | `testing` |
| -- | `voice-state-machine` |
| -- | `xstate` |
| Building design system | `atomic-design` |
| Building with EAS | `expo` |
| Configuring app.json or app.config.js | `expo` |
| Creating React Native components | `react-native` |
| Creating React components | `react` |
| Creating UI components | `atomic-design` |
| Creating new features | `clean-architecture` |
| Creating speech recognition flows | `voice-state-machine` |
| Creating state machines | `xstate` |
| Handling mobile-specific APIs | `react-native` |
| Image classification tasks | `tensorflow-lite` |
| Implementing TDD | `testing` |
| Implementing actor patterns | `xstate` |
| Implementing object detection | `tensorflow-lite` |
| Implementing voice commands | `voice-state-machine` |
| Managing complex state flows | `xstate` |
| Managing component state | `react` |
| Mocking dependencies | `testing` |
| Organizing component hierarchy | `atomic-design` |
| Separating business logic from UI | `clean-architecture` |
| Setting up test infrastructure | `testing` |
| Structuring project architecture | `clean-architecture` |
| Using Expo SDK modules | `expo` |
| Working with ML models | `tensorflow-lite` |
| Working with React hooks | `react` |
| Working with mobile navigation | `react-native` |
| Working with wake word detection | `voice-state-machine` |
| Writing tests | `testing` |

---

## Tech Stack

| Layer | Technology | Version | Purpose |
|-------|------------|---------|---------|
| Framework | React Native | 0.81.5 | Cross-platform mobile |
| Platform | Expo SDK | 54 | Development & build tooling |
| Runtime | React | 19.1.0 | UI library |
| Vision AI | TensorFlow Lite | - | On-device object detection |
| Models | COCO-SSD, MobileNet | - | Detection + classification |
| Voice Input | `@react-native-voice/voice` | - | Speech recognition |
| Voice Output | `expo-speech` | - | Text-to-speech |
| Camera | `expo-camera` | - | Image capture |
| Feedback | `expo-haptics` | - | Tactile responses |

> **Note**: Using Expo SDK 54 with React 19 and the New Architecture enabled.

---

## Project Structure

Screaming Architecture + Clean Architecture + Atomic Design:

```
mobile/
├── App.js                          # Entry point
├── app.config.js                   # Expo configuration
│
└── src/
    │
    ├── voice/                      # FEATURE: Voice commands
    │   ├── domain/
    │   │   ├── entities/
    │   │   │   └── VoiceCommand.js
    │   │   ├── repositories/
    │   │   │   └── VoiceRepository.js      # Interface
    │   │   └── value-objects/
    │   │       └── CommandIntent.js
    │   ├── application/
    │   │   ├── use-cases/
    │   │   │   ├── ProcessCommand.js
    │   │   │   └── ListenForWakeWord.js
    │   │   └── ports/
    │   │       └── SpeechRecognizer.js     # Interface
    │   ├── infrastructure/
    │   │   └── adapters/
    │   │       ├── RNVoiceAdapter.js
    │   │       └── ExpoSpeechAdapter.js
    │   └── presentation/
    │       ├── hooks/
    │       │   └── useVoiceCommands.js
    │       └── components/
    │           └── organisms/
    │               └── VoiceCommandPanel.jsx
    │
    ├── vision/                     # FEATURE: Scene analysis
    │   ├── domain/
    │   │   ├── entities/
    │   │   │   ├── SceneDescription.js
    │   │   │   └── DetectedObject.js
    │   │   ├── repositories/
    │   │   │   └── DescriptionRepository.js
    │   │   └── services/
    │   │       └── DescriptionBuilder.js   # Spanish natural language
    │   ├── application/
    │   │   ├── use-cases/
    │   │   │   ├── AnalyzeScene.js
    │   │   │   └── RepeatLastDescription.js
    │   │   └── ports/
    │   │       └── VisionAnalyzer.js       # Interface
    │   ├── infrastructure/
    │   │   ├── adapters/
    │   │   │   └── TFLiteVisionAdapter.js
    │   │   └── mappers/
    │   │       └── CocoLabelMapper.js      # English -> Spanish
    │   └── presentation/
    │       ├── hooks/
    │       │   └── useVisionAnalysis.js
    │       └── components/
    │           └── organisms/
    │               ├── CameraPreview.jsx
    │               └── DescriptionCard.jsx
    │
    └── shared/                     # Cross-cutting concerns
        ├── domain/
        │   └── events/
        │       └── DomainEvent.js
        ├── infrastructure/
        │   └── config/
        │       └── IrisConfig.js           # Voice settings, thresholds
        ├── di/
        │   └── container.js                # Dependency injection
        └── presentation/
            └── components/
                ├── atoms/
                │   ├── Button.jsx
                │   ├── Icon.jsx
                │   ├── Typography.jsx
                │   └── index.js
                ├── molecules/
                │   ├── IconButton.jsx
                │   ├── StatusIndicator.jsx
                │   └── index.js
                ├── organisms/
                │   └── index.js
                ├── templates/
                │   ├── MainLayout.jsx
                │   ├── CameraLayout.jsx
                │   └── index.js
                └── pages/
                    └── HomeScreen.jsx
```

---

## Development Commands

```bash
# Install dependencies (use bun + expo install for SDK packages)
bun install
bunx expo install expo-camera expo-speech expo-haptics

# Start development
bun start

# Run on device/simulator
bun run ios
bun run android

# Clear cache
bun run clear

# Build for testing
eas build --profile preview --platform ios
eas build --profile preview --platform android
```

> **Package Manager**: This project uses `bun` for faster installs. Always use `bunx expo install` for Expo SDK packages to ensure version compatibility.

---

## Architecture Rules

### Layer Dependencies

```
Presentation → Application → Domain ← Infrastructure

✓ Presentation imports Application (use cases)
✓ Application imports Domain (entities)
✓ Infrastructure implements Domain interfaces
✗ Domain NEVER imports from other layers
```

### File Placement Decision Tree

```
Is it a business rule or entity?           → domain/
Is it orchestrating multiple operations?   → application/use-cases/
Is it an external service integration?     → infrastructure/adapters/
Is it a React component?                   → presentation/components/
Is it a React hook consuming use cases?    → presentation/hooks/
Is it shared across features?              → shared/
```

### Atomic Design Placement

```
Basic UI element (Button, Text)?           → atoms/
Combination of 2-3 atoms?                  → molecules/
Complex self-contained section?            → organisms/
Page layout without data?                  → templates/
Complete screen with logic?                → pages/
```

---

## Code Patterns

### Domain Entity

```javascript
// domain/entities/VoiceCommand.js
export class VoiceCommand {
  constructor({ text, confidence, timestamp }) {
    this.text = text;
    this.confidence = confidence;
    this.timestamp = timestamp;
  }

  isValid() {
    return this.confidence > 0.7;
  }

  containsWakeWord(wakeWord = 'iris') {
    return this.text.toLowerCase().includes(wakeWord);
  }
}
```

### Use Case

```javascript
// application/use-cases/AnalyzeScene.js
export class AnalyzeSceneUseCase {
  constructor({ visionAnalyzer, descriptionBuilder, speechSynthesizer }) {
    this.visionAnalyzer = visionAnalyzer;
    this.descriptionBuilder = descriptionBuilder;
    this.speechSynthesizer = speechSynthesizer;
  }

  async execute(imageData) {
    const objects = await this.visionAnalyzer.detect(imageData);
    const description = this.descriptionBuilder.build(objects);
    await this.speechSynthesizer.speak(description);
    return { objects, description };
  }
}
```

### Infrastructure Adapter

```javascript
// infrastructure/adapters/TFLiteVisionAdapter.js
import * as cocoSsd from '@tensorflow-models/coco-ssd';
import { VisionAnalyzer } from '../../application/ports/VisionAnalyzer';

export class TFLiteVisionAdapter extends VisionAnalyzer {
  async detect(imageTensor) {
    const predictions = await this.model.detect(imageTensor);
    return predictions.filter(p => p.score > 0.6);
  }
}
```

### Presentation Hook

```javascript
// presentation/hooks/useVisionAnalysis.js
export function useVisionAnalysis() {
  const { visionAnalyzer, descriptionBuilder, speechSynthesizer } = useDependencies();
  const analyzeScene = useMemo(
    () => new AnalyzeSceneUseCase({ visionAnalyzer, descriptionBuilder, speechSynthesizer }),
    [visionAnalyzer, descriptionBuilder, speechSynthesizer]
  );
  // ...
}
```

### Atom Component

```javascript
// shared/presentation/components/atoms/Button.jsx
export function Button({ label, onPress, variant = 'primary' }) {
  return (
    <TouchableOpacity
      style={[styles.button, styles[variant]]}
      onPress={onPress}
      accessible={true}
      accessibilityRole="button"
      accessibilityLabel={label}
    >
      <Typography>{label}</Typography>
    </TouchableOpacity>
  );
}
```

---

## Accessibility Requirements

Iris is built for visually impaired users. **Every interactive element MUST have:**

```javascript
<TouchableOpacity
  accessible={true}
  accessibilityLabel="Clear description of element"
  accessibilityHint="What happens when activated"
  accessibilityRole="button"
>
```

### Voice Feedback Rules

1. **Always confirm actions** - "Photo captured", "Analyzing..."
2. **Announce errors clearly** - "Camera not available"
3. **Keep descriptions concise** - Focus on most relevant objects
4. **Use natural Spanish** - "Veo una persona y una silla" not "Detectado: persona, silla"

---

## Performance Guidelines

| Area | Rule |
|------|------|
| ML Inference | Use `lite_mobilenet_v2` for speed |
| Memory | Always `dispose()` tensors after use |
| Camera | Process frames at 300x300 max |
| Lists | Use `FlatList` with `keyExtractor` |
| Re-renders | Memoize callbacks with `useCallback` |

---

## Testing

```bash
# Lint
npx eslint .

# Type check (if using TypeScript)
npx tsc --noEmit

# Test on device (required for camera/voice)
npx expo run:ios --device
```

**Note:** Camera and voice features cannot be tested in simulators. Always test on physical devices.
